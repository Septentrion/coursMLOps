{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "025b0467",
   "metadata": {},
   "source": [
    "# Jeu d'échecs par renforcement I\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Le jeu d'échecs a toujours été un cas emblématique des algorithmes d'intelligence artificielle, d'une part parce que :\n",
    "\n",
    "1. Ses règles sont bien établies et non ambiguës ;\n",
    "2. La dimension du problème rend impossible une résolution « fonctionnelle »\n",
    "3. On a longtemps pensé (années 196x - 197x) que c'était un problèpe simple à résoudre par une approche exploratoire.\n",
    "\n",
    "\n",
    "L'objectif est d'implémenter un jeu d'échecs en choisissant une méthode d'apprentissage par renforcement, c'est-à-dire une méthode qui ne nécessite pas de modèle pré-établi.\n",
    "\n",
    "Dans notre cas, nous n'aurons donc pas besoin de collecter une bibliothèque de parties qu nous permettraient de montrer au réseau de neurones quelles sont celles dont la stratégie est gagnante et celles où elle est perdante.\n",
    "\n",
    "L'apprentissage par renforcement (AR) va permettre au réseau d'apprendre quels sont les coups qui la « plus grande valeur »  dans un contexte donné et de déterminer le meilleur « chemin » vers le gain de la partie. \n",
    "\n",
    "D'une certaine manière, AR est l'héritier d'algorithmes classiques comme MinMax. Mais le problème de ces algorithmes est que la valeur de certaines décisions est déterminée _a priori_ et qu'il faut écrire des fonctions, dites **heuristiques**, qui permettent au programme de calculer la valeur attribuée à une décision. Cequi est naturellement un problème très difficile. AR vise à résoudre ce problème en laissant le réseau de neurones apprendre par lui-même la fonction heuristique. En conséquence, celle-ci pourra dépendre d'un nombre arbitrairement grand de paramètres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb3982c",
   "metadata": {},
   "source": [
    "## Etape 1 : Environnement\n",
    "\n",
    "Nos avons besoin, naturellement, de représenter les éléments du jeu : \n",
    "- l'échiquier\n",
    "- les pièces\n",
    "- la partie\n",
    "\n",
    "\n",
    "### L'échiquier\n",
    "\n",
    "L'échiquier nous permet simplment de définir un espace de déplacement pour les pièces, avec en particulier un point de départ et un point d'arrivée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e40ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Board :\n",
    "    \"\"\"\n",
    "    Un échiquier\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Position initiale\n",
    "        self.state = (0, 0)\n",
    "        # Position finale\n",
    "        self.terminal_state = (7, 5)\n",
    "        # Espace des récompenses\n",
    "        # Tous les déplacements sont équivalents et reçoivent une récompense de -1\n",
    "        self.reward_space = np.zeros(shape=(8, 8)) - 1\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Calcul de la récompense liée à un déplacement vers une nouvelle position\n",
    "        \n",
    "        :param action: Le déplacement opéré par un agent\n",
    "        :type action:  tuple\n",
    "        \n",
    "        :return:       La récompsense pour le déplacement\n",
    "        :rtype:        tuple\n",
    "        \"\"\"\n",
    "        reward = self.reward_space[self.state[0], self.state[1]]\n",
    "        if self.state == self.terminal_state:\n",
    "            episode_end = True\n",
    "            return 0, episode_end\n",
    "        else:\n",
    "            episode_end = False\n",
    "            old_state = self.state\n",
    "            new_state = (self.state[0] + action[0], self.state[1] + action[1])\n",
    "            self.state = old_state if np.min(new_state) < 0 or np.max(new_state) > 7 else new_state\n",
    "            return reward, episode_end\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Rendu visuel de l'échiquier dans le terminal\n",
    "        \"\"\"\n",
    "        visual_row = [\"[ ]\", \"[ ]\", \"[ ]\", \"[ ]\", \"[ ]\", \"[ ]\", \"[ ]\", \"[ ]\"]\n",
    "        visual_board = []\n",
    "        for c in range(8):\n",
    "            visual_board.append(visual_row.copy())\n",
    "        visual_board[self.state[0]][self.state[1]] = \"[S]\"\n",
    "        visual_board[self.terminal_state[0]][self.terminal_state[1]] = \"[F]\"\n",
    "        self.visual_board = visual_board\n",
    "        \n",
    "# Test\n",
    "board= Board()\n",
    "board.render()\n",
    "board.visual_board "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4580ab",
   "metadata": {},
   "source": [
    "\n",
    "### Les pièces \n",
    "\n",
    "Une pièce du jeu d’échecs, qui peut être : Roi, Fou, Cavalier, Tour.\n",
    "\n",
    "Pour le moment, nous avons juste besoin de définir l'espace des mouvements possibles de ces pièces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e192c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class Piece :\n",
    "    \"\"\"\n",
    "    Une pièce du jeu d'écheccs\n",
    "    \"\"\"\n",
    "    def __init__(self, piece=\"King\"):\n",
    "        self.piece = piece\n",
    "        self.init_actionspace()\n",
    "\n",
    "\n",
    "    def init_actionspace(self):\n",
    "        \"\"\"\n",
    "        Définition des mouvements possibles pour chaque pièce du jeu\n",
    "        \"\"\"\n",
    "        assert self.piece in [\"King\", \"Rook\", \"Bishop\", \"Knight\"], f\"{self.piece} n'est pas une pièce autorisée\"\n",
    "        if self.piece == 'King':\n",
    "            self.action_space = [(-1, 0),  # north\n",
    "                                 (-1, 1),  # north-west\n",
    "                                 (0, 1),  # west\n",
    "                                 (1, 1),  # south-west\n",
    "                                 (1, 0),  # south\n",
    "                                 (1, -1),  # south-east\n",
    "                                 (0, -1),  # east\n",
    "                                 (-1, -1),  # north-east\n",
    "                                 ]\n",
    "        elif self.piece == 'Rook':\n",
    "            self.action_space = []\n",
    "            for amplitude in range(1, 8):\n",
    "                self.action_space.append((-amplitude, 0))  # north\n",
    "                self.action_space.append((0, amplitude))  # east\n",
    "                self.action_space.append((amplitude, 0))  # south\n",
    "                self.action_space.append((0, -amplitude))  # west\n",
    "        elif self.piece == 'Knight':\n",
    "            self.action_space = [(-2, 1),  # north-north-west\n",
    "                                 (-1, 2),  # n-w-w\n",
    "                                 (1, 2),  # s-w-w\n",
    "                                 (2, 1),  # s-s-w\n",
    "                                 (2, -1),  # s-s-e\n",
    "                                 (1, -2),  # s-e-e\n",
    "                                 (-1, -2),  # n-e-e\n",
    "                                 (-2, -1)]  # n-n-e\n",
    "        elif self.piece == 'Bishop':\n",
    "            self.action_space = []\n",
    "            for amplitude in range(1, 8):\n",
    "                self.action_space.append((-amplitude, amplitude))  # north-west\n",
    "                self.action_space.append((amplitude, amplitude))  # south-west\n",
    "                self.action_space.append((amplitude, -amplitude))  # south-east\n",
    "                self.action_space.append((-amplitude, -amplitude))  # north\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8eea0",
   "metadata": {},
   "source": [
    "### La partie\n",
    "\n",
    "La partie est l'espace de calcul où évoluent les pièces du jeu d'échecs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c124d63",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "class Reinforce :\n",
    "    \"\"\"\n",
    "    Une partie\n",
    "    \"\"\"\n",
    "    def __init__(self, agent, environment) :\n",
    "        self.env = environment\n",
    "        self.agent = agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be054cf5",
   "metadata": {},
   "source": [
    "## Etape 2 : Déplacer les pièces\n",
    "\n",
    "Dans un premier, nos allons apprendre à déplacer les pièces sur l'échiquier. Pour cela, nous posons les hypothèses suivantes :\n",
    "- L'espace des états est un tableau à deux dimensions de 8 x 8\n",
    "- Nous partons d'un état initial **S** aux coordonnées (0, 0)\n",
    "- Nous choisissons unétat final **F** aux coordonnées (7,5)\n",
    "- Chaque déplacement engendre une « récompense » de -1\n",
    "- La meilleure stratégie est celle qui permet de se déplace se S à F en un minimum de coups\n",
    "\n",
    "### Evaluation de la position\n",
    "\n",
    "Le principe de AR est de faire en sorte qu'un « agent » cherche à optimiser ses récompenses. Le bu de l'apprentissage est donc de lui permettre de se diriger vers les positions qui lui permettront de maximiser celles-ci. Chaque position est associé à une certaine valeur. D'où :\n",
    "\n",
    "- Une position (P) a une valeur (V) égale à la valeur (V’) de la position précédente (P’) plus la récompense (R)\n",
    "- Comme plusieurs décisions sont possibles à chaque position (P), nous faisons la somme des valeurs multipliées par leur probabilité\n",
    "- Les valeurs des positions suivantess sont pondérées par  un facteur (gamma) variant entre 0 et 1.\n",
    "- La valeur pour unétat suivant est une estimation\n",
    "- Evaluer une position est un processus récursif : on calcule une estimation basée sur d'autres estimations\n",
    "\n",
    "Nous pouvons donc écrire la fonction d'évaluation suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c651cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Reinforce :\n",
    "    \n",
    "    def evaluate_state(self, state, gamma=0.9, synchronous=True):\n",
    "            \"\"\"\n",
    "            Calcule la valeur d'une position en fonction des valeurs des positions ultérieures (suivantes).\n",
    "            Args:\n",
    "                state: Les coordonnées de la position sur l'échiquier\n",
    "                gamma: Le facteur de pondération\n",
    "                synchronous: [...]\n",
    "\n",
    "            Returns: La valeur de la position (étant donnée une certaine stratégie).\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            # Valuer optimale\n",
    "            greedy_action_value = np.max(self.agent.policy[state[0], state[1], :])\n",
    "\n",
    "            # Liste des mouvements possibles\n",
    "            greedy_indices = [i for i, a in enumerate(self.agent.policy[state[0], state[1], :]) if\n",
    "                              a == greedy_action_value]\n",
    "            \n",
    "            # Probabilité d'occurrence d'un mouvement\n",
    "            prob = 1 / len(greedy_indices)\n",
    "            \n",
    "            # Valeur initiale\n",
    "            state_value = 0\n",
    "            \n",
    "            # Itération sur tous les mouvements possibles\n",
    "            for i in greedy_indices:\n",
    "                # Actualisation de la position\n",
    "                self.env.state = state\n",
    "                # Calcul de la récompnse pour un mouvement possible\n",
    "                reward, episode_end = self.env.step(self.agent.action_space[i])\n",
    "                #if synchronous:\n",
    "                    successor_state_value = self.agent.value_function_prev[self.env.state]\n",
    "                #else:\n",
    "                #    successor_state_value = self.agent.value_function[self.env.state]\n",
    "                # Actualisation de la valeur de la position actuelle\n",
    "                state_value += (prob * (reward + gamma * successor_state_value))\n",
    "                \n",
    "            # Valeur estimée de la position\n",
    "            return state_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e3ae30",
   "metadata": {},
   "source": [
    "### Evaluation de la stratégie\n",
    "\n",
    "L'évaluation de la stratégie est simplement l'évaluation des positions pour tout l'espace des positions. Nous devons donc juste répéter la fonction `evalutate_state` pour toutes les positions possibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edf1ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Reinforce :\n",
    "    \n",
    "    def evaluate_policy(self, gamma=0.9, synchronous=True) :\n",
    "        # Sauvegarde de la position précédente\n",
    "        self.agent.value_function_prev = self.agent.value_function.copy()\n",
    "        \n",
    "        # Itération sur toutes les cases de l'échiquier\n",
    "        for row in range(self.agent.value_function.shape[0]):\n",
    "            for col in range(self.agent.value_function.shape[1]):\n",
    "                self.agent.value_function[row, col] = self.evaluate_state((row, col), gamma=gamma, synchronous=synchronous)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc2e87c",
   "metadata": {},
   "source": [
    "Ceci étant posé, nouspouvons maintenant apprendre à reconnaître le meileur chemin en répétant l'évaluation jusqu'à ce que les valeurs soient stables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316c69f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisation du système\n",
    "r = Reinfoce(Borad(), Piece(pice=\"King\"))\n",
    "# Définition d'un seuil \n",
    "epsilon=0.1\n",
    "# Nombre d'itération maximal\n",
    "k_max = 1000\n",
    "# Ecart maximal\n",
    "value_delta_max = 0\n",
    "# Facteur de pondération\n",
    "gamma = 1\n",
    "# Evaluation synchrone\n",
    "synchronous=True\n",
    "for k in range(k_max):\n",
    "    r.evaluate_policy(gamma=gamma,synchronous=synchronous)\n",
    "    value_delta = np.max(np.abs(r.agent.value_function_prev - r.agent.value_function))\n",
    "    value_delta_max = value_delta\n",
    "    if value_delta_max < eps:\n",
    "        print('converged at iter',k)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a1508d",
   "metadata": {},
   "source": [
    "### Optimisation de la stratégie\n",
    "\n",
    "Maintenant que nous connaissons les valeurs des positions, ce qui nous intéresse est de guider notre « agent » vers la meilleure position possible, celle avec la valeur la plus grande. L'optimisation de la stratégie consiste simplement à instaurer un comportement « impatient » (ou « avide ») de l'agent, relativement à la valeur de chaque position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9412183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Reinforce :\n",
    "    \n",
    "        def improve_policy(self):\n",
    "        \"\"\"\n",
    "        Trouve la stratégie « impatiente » en regard de l'évaluation des positions.\n",
    "        \"\"\"\n",
    "\n",
    "        self.agent.policy_prev = self.agent.policy.copy()\n",
    "        for row in range(self.agent.action_function.shape[0]):\n",
    "            for col in range(self.agent.action_function.shape[1]):\n",
    "                for action in range(self.agent.action_function.shape[2]):\n",
    "                    self.env.state = (row, col)  # reset state to the one being evaluated\n",
    "                    reward, episode_end = self.env.step(self.agent.action_space[action])\n",
    "                    successor_state_value = 0 if episode_end else self.agent.value_function[self.env.state]\n",
    "                    self.agent.policy[row, col, action] = reward + successor_state_value\n",
    "\n",
    "                max_policy_value = np.max(self.agent.policy[row, col, :])\n",
    "                max_indices = [i for i, a in enumerate(self.agent.policy[row, col, :]) if a == max_policy_value]\n",
    "                for idx in max_indices:\n",
    "                    self.agent.policy[row, col, idx] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f7da3",
   "metadata": {},
   "source": [
    "De la même manière que précédemment, nous pouvons maintenant apprendre la meilleure trajectoire en itérant la fonction `improve_policy` jusqu'à ce que les valeurs soient stables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b2519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reinforce :\n",
    "    \n",
    "        def policy_iteration(self, eps=0.1, gamma=0.9, iteration=1, k=32, synchronous=True):\n",
    "        \"\"\"\n",
    "        Trouve la stratégie optimale\n",
    "\n",
    "        :param epsilon: \n",
    "        :type epsilon: float\n",
    "        :para gamma: \n",
    "        :type gamma: float\n",
    "        : param iteration:\n",
    "        :type iteration: int\n",
    "        :param k:\n",
    "        :type k: int\n",
    "        :param synchronous:\n",
    "        :type synchronous: boolean\n",
    "\n",
    "        :return:\n",
    "        :rtype: void\n",
    "\n",
    "        \"\"\"\n",
    "        policy_stable = True\n",
    "        print(\"\\n\\n______iteration:\", iteration, \"______\")\n",
    "        print(\"\\n policy:\")\n",
    "        self.visualize_policy()\n",
    "\n",
    "        print(\"\")\n",
    "        value_delta_max = 0\n",
    "        for _ in range(k):\n",
    "            self.evaluate_policy(gamma=gamma, synchronous=synchronous)\n",
    "            value_delta = np.max(np.abs(self.agent.value_function_prev - self.agent.value_function))\n",
    "            value_delta_max = value_delta\n",
    "            if value_delta_max < eps:\n",
    "                break\n",
    "        print(\"Value function for this policy:\")\n",
    "        print(self.agent.value_function.round().astype(int))\n",
    "        action_function_prev = self.agent.action_function.copy()\n",
    "        print(\"\\n Improving policy:\")\n",
    "        self.improve_policy()\n",
    "        policy_stable = self.agent.compare_policies() < 1\n",
    "        print(\"policy diff:\", policy_stable)\n",
    "\n",
    "        if not policy_stable and iteration < 1000:\n",
    "            iteration += 1\n",
    "            self.policy_iteration(iteration=iteration)\n",
    "        elif policy_stable:\n",
    "            print(\"Optimal policy found in\", iteration, \"steps of policy evaluation\")\n",
    "        else:\n",
    "            print(\"failed to converge.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d71c32e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
